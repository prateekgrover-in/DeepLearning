{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16: CIFAR - 100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhVnoK9HnZ9X",
        "colab_type": "text"
      },
      "source": [
        "# Step-by-step approach to training a DeepNeuralNetwork for ImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfr3xtHQhvjo",
        "colab_type": "text"
      },
      "source": [
        "We'll be working with Keras, a deep learning library, to implement a deep Convolutional Neural Network taking the CIFAR - 100 Dataset, hoping to achieve test accuracy of more than 65%, gradually moving from basic elements to advanced elements, observing the training procedure throughout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9STb6B6rocHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fdfb9bb-3653-4ae0-9d52-fadc5bf64d7c"
      },
      "source": [
        "import numpy as np\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jtxysUoopb5",
        "colab_type": "text"
      },
      "source": [
        "# Understanding the CIFAR - 100 Dataset\n",
        "CIFAR - 100 Dataset contains 60000 coloured images of dimensions 32x32, each labelled to one of the 100 classes that all of them belong to. The dataset is provided in the Keras library itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1VUc0kSpxWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe-KTUFipvuy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Using the load_data(), the dataset is divided into training and testing data function, that shall be used to evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Bq30VGo6P6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "db976241-6a41-45f5-938d-2daf640fa549"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar100.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Fi_BQSuDPd",
        "colab_type": "text"
      },
      "source": [
        "We shall also define variables, num_label_classes defining the number of label classes and input_shape indicating the input 3-D Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OIJwKFZt5MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_label_classes = 100\n",
        "input_image_shape = [32,32,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jmgMUSLqao9",
        "colab_type": "text"
      },
      "source": [
        "Let's look at how the divison of data takes place using this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqG5Ap8RqfMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3c1294d0-7c34-4183-d4f5-a4c6691ee388"
      },
      "source": [
        "print(\"The number of images available for training and validation are: \" + str(train_images.shape[0]))\n",
        "print(\"while, the number of images for testing are: \" + str(test_images.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of images available for training and validation are: 50000\n",
            "while, the number of images for testing are: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wip0FU7q_p4",
        "colab_type": "text"
      },
      "source": [
        "So, let us now look at a few examples from the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmFqn1HLpPOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "4266b21d-36b3-4f55-8fde-f4be84290717"
      },
      "source": [
        "import matplotlib.pyplot as plt     #importing the graphing library\n",
        "\n",
        "plt.figure(figsize=(10,2))\n",
        "for i in range(5):\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i].reshape(32, 32,3), cmap=plt.cm.RdYlBu)\n",
        "    plt.xlabel(train_labels[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if s != self._text:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9XBPjbqEcOt",
        "colab_type": "text"
      },
      "source": [
        "#Dataset Preprocessing\n",
        "Our objective shall be to transform the input images into a 3D array giving the illusion of a normal distribution ie zero mean and unit standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY8Y9NuOEiVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting the array elements to float datatype\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zELRdiOkEnPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.mean (train_images, axis=(0, 1, 2, 3))\n",
        "std = np.std (train_images, axis=(0, 1, 2, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HE1PF-tErOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performing the mathematical formula throughout the arrays\n",
        "train_images = (train_images-mean)/(std+1e-7)\n",
        "test_images = (test_images-mean)/(std+1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45dlOS4aF0PD",
        "colab_type": "text"
      },
      "source": [
        "We shall also change the categorical labels to one-hot-encoding, so as to calculate loss easily. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Run-n7ruGYRH",
        "colab_type": "text"
      },
      "source": [
        "*Eg: a label of 12 shall have value of 1 for the twelfth position of array of labels while rest of the array elements will be zero.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oA7KRMYEsGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = keras.utils.to_categorical(train_labels, num_label_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_label_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-v4aiECucfi",
        "colab_type": "text"
      },
      "source": [
        "# Creating the VGG-16 Neural Network\n",
        "\n",
        "We shall be forming our deep CNN through the Sequential() model provided by Keras, this shall involve an API for a number of layers :\n",
        "\n",
        "*   ***Conv2D*** : provides a convolution layer with defined size and stride\n",
        "\n",
        "*   ***MaxPooling2D*** : forms Layer B by downsampling Layer A, taking the maximum value over the window defined by pool_size for each dimension along the features axis. \n",
        "\n",
        "*   ***BatchNormalization*** : Normalize the activations of the Layer A at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
        "\n",
        "*   ***Dense*** : provides a fully connected layer from Layer A to Layer B\n",
        "\n",
        "*   ***Dropout*** : randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "\n",
        "*   ***Activation*** : Applies an activation function like ReLu or Sigmoid to an output.\n",
        "\n",
        "*   ***Flatten*** : Flattens the input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pJxtk7eCM8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjrQRXy8bcNd",
        "colab_type": "text"
      },
      "source": [
        "> ![alt text](https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg)\n",
        "\n",
        "Paper: [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "Karen Simonyan & Andrew Zisserman\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2UhDO6yGrpI",
        "colab_type": "text"
      },
      "source": [
        "To reduce overfitting, we shall also add L2 Kernel Regularizer that shall allow you to apply penalties on layer parameters during optimization. These penalties are summed into the loss function that the network optimizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QPv4sEVGsVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "weight_decay = 0.0005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfhdGHl2unWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_image_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_label_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H70exmeeEB7F",
        "colab_type": "text"
      },
      "source": [
        "Let's look at how the model shapes out layer by layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mZpvhagENc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d17e101-3908-48ed-f918-49ab18a675a0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               51300     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 15,047,588\n",
            "Trainable params: 15,038,116\n",
            "Non-trainable params: 9,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr3UQCoYmUvf",
        "colab_type": "text"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsczZDFHGou",
        "colab_type": "text"
      },
      "source": [
        "Since the amount of data available for us is quite less to train, we shall try to augment the given data to generate variations (horizontally flipped, zoomed in, rotated) of the same images that'll help our model become more robust."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcNMngGNHngn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SSqnafYIXQG",
        "colab_type": "text"
      },
      "source": [
        "We shall use ImageDataGenerator for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDCCsCXLl8FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_augmentation = ImageDataGenerator(\n",
        "                            rotation_range=15,  # randomly rotates images in the range (degrees, 0 to 180)\n",
        "                            width_shift_range=0.1,  # randomly shifts images horizontally (fraction of total width)\n",
        "                            height_shift_range=0.1,  # randomly shifts images vertically (fraction of total height)\n",
        "                            horizontal_flip=True)  # randomly flips images horizontally\n",
        "\n",
        "generator_augmentation.fit(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYXMaaoyItBR",
        "colab_type": "text"
      },
      "source": [
        "#Training the Model\n",
        "We shall start off with defining LearningRateScheduler Callback for the training process, where the learning rate shall decrease by 0.5 after every 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twoBoRUbJe6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_drop = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p01qcFzI1l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learning_rate_scheduler(epoch):\n",
        "    return learning_rate * (0.5 ** (epoch // learning_rate_drop))\n",
        "\n",
        "reduce_learning_rate = keras.callbacks.LearningRateScheduler(learning_rate_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkLA69zFJ3gY",
        "colab_type": "text"
      },
      "source": [
        "Stochastic Gradient Descent with Momentum shall be used as the process to converge to the minima. The initial learning rate shall be 0.1 with the decay of learning rate as 1e-6 throughout along with the callback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQScy00sKJ1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "learning_rate_decay = 1e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLY_7oINJ2EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(lr=learning_rate, decay=learning_rate_decay, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoEd-z9mRDal",
        "colab_type": "text"
      },
      "source": [
        "We, next create a validation set from the training set, to assess the model as the training goes on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2pBHkWXVP1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 0.8\n",
        "train_images_input = train_images[:int(train_images.shape[0]*x)]\n",
        "train_labels_input = train_labels[:int(train_images.shape[0]*x)]\n",
        "valid_images = train_images[int(train_images.shape[0]*x): train_images.shape[0]]\n",
        "valid_labels = train_labels[int(train_labels.shape[0]*x): train_labels.shape[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz-ZQZYhFcpV",
        "colab_type": "text"
      },
      "source": [
        "fit_generator is used to train our model, with batch sze of 128 and the total number of epochs are 250."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh_qn_3HLEas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5f88d0e-ca02-4f83-b69f-45ccef3c6d89"
      },
      "source": [
        "history_training = model.fit_generator((generator_augmentation.flow(train_images_input,train_labels_input, batch_size=128)),\n",
        "                                      steps_per_epoch=train_images.shape[0] // 128,\n",
        "                                      epochs=250,\n",
        "                                      validation_data=(valid_images, valid_labels),\n",
        "                                      callbacks=[reduce_learning_rate],\n",
        "                                      verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 18.8892 - accuracy: 0.0238 - val_loss: 14.6801 - val_accuracy: 0.0089\n",
            "Epoch 2/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 10.9318 - accuracy: 0.0434 - val_loss: 9.0100 - val_accuracy: 0.0255\n",
            "Epoch 3/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 7.2173 - accuracy: 0.0591 - val_loss: 6.8568 - val_accuracy: 0.0201\n",
            "Epoch 4/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 5.5167 - accuracy: 0.0795 - val_loss: 5.3102 - val_accuracy: 0.0608\n",
            "Epoch 5/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 4.6899 - accuracy: 0.0990 - val_loss: 4.5521 - val_accuracy: 0.1027\n",
            "Epoch 6/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 4.2856 - accuracy: 0.1208 - val_loss: 4.1783 - val_accuracy: 0.1311\n",
            "Epoch 7/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 4.0578 - accuracy: 0.1441 - val_loss: 3.9890 - val_accuracy: 0.1577\n",
            "Epoch 8/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.9130 - accuracy: 0.1717 - val_loss: 3.8805 - val_accuracy: 0.1858\n",
            "Epoch 9/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.8084 - accuracy: 0.1968 - val_loss: 3.7175 - val_accuracy: 0.2216\n",
            "Epoch 10/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7579 - accuracy: 0.2198 - val_loss: 3.6531 - val_accuracy: 0.2491\n",
            "Epoch 11/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7267 - accuracy: 0.2401 - val_loss: 3.8401 - val_accuracy: 0.2491\n",
            "Epoch 12/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7042 - accuracy: 0.2541 - val_loss: 3.8208 - val_accuracy: 0.2642\n",
            "Epoch 13/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.6905 - accuracy: 0.2728 - val_loss: 3.5964 - val_accuracy: 0.2908\n",
            "Epoch 14/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7101 - accuracy: 0.2822 - val_loss: 3.7734 - val_accuracy: 0.2832\n",
            "Epoch 15/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 3.7207 - accuracy: 0.2929 - val_loss: 3.5959 - val_accuracy: 0.3290\n",
            "Epoch 16/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7381 - accuracy: 0.3010 - val_loss: 3.8760 - val_accuracy: 0.3071\n",
            "Epoch 17/250\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 3.7452 - accuracy: 0.3085 - val_loss: 3.6401 - val_accuracy: 0.3388\n",
            "Epoch 18/250\n",
            "390/390 [==============================] - 89s 229ms/step - loss: 3.7600 - accuracy: 0.3183 - val_loss: 3.9219 - val_accuracy: 0.2995\n",
            "Epoch 19/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 3.7660 - accuracy: 0.3241 - val_loss: 3.9057 - val_accuracy: 0.3217\n",
            "Epoch 20/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.7824 - accuracy: 0.3320 - val_loss: 3.8901 - val_accuracy: 0.3297\n",
            "Epoch 21/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.4406 - accuracy: 0.3914 - val_loss: 3.4205 - val_accuracy: 0.3904\n",
            "Epoch 22/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 3.2810 - accuracy: 0.4067 - val_loss: 3.3229 - val_accuracy: 0.3952\n",
            "Epoch 23/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2445 - accuracy: 0.4082 - val_loss: 3.1658 - val_accuracy: 0.4232\n",
            "Epoch 24/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2537 - accuracy: 0.4121 - val_loss: 3.4686 - val_accuracy: 0.3932\n",
            "Epoch 25/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2309 - accuracy: 0.4226 - val_loss: 3.2181 - val_accuracy: 0.4369\n",
            "Epoch 26/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2599 - accuracy: 0.4209 - val_loss: 3.1356 - val_accuracy: 0.4535\n",
            "Epoch 27/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2483 - accuracy: 0.4291 - val_loss: 3.2557 - val_accuracy: 0.4450\n",
            "Epoch 28/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2654 - accuracy: 0.4317 - val_loss: 3.2708 - val_accuracy: 0.4432\n",
            "Epoch 29/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2692 - accuracy: 0.4377 - val_loss: 3.5291 - val_accuracy: 0.4145\n",
            "Epoch 30/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 3.2731 - accuracy: 0.4415 - val_loss: 3.2097 - val_accuracy: 0.4705\n",
            "Epoch 31/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 3.2870 - accuracy: 0.4446 - val_loss: 3.4027 - val_accuracy: 0.4332\n",
            "Epoch 32/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2961 - accuracy: 0.4457 - val_loss: 3.2239 - val_accuracy: 0.4661\n",
            "Epoch 33/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2944 - accuracy: 0.4514 - val_loss: 3.5543 - val_accuracy: 0.4323\n",
            "Epoch 34/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.2952 - accuracy: 0.4584 - val_loss: 3.3982 - val_accuracy: 0.4533\n",
            "Epoch 35/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.3031 - accuracy: 0.4585 - val_loss: 3.3066 - val_accuracy: 0.4624\n",
            "Epoch 36/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.3298 - accuracy: 0.4571 - val_loss: 3.3599 - val_accuracy: 0.4568\n",
            "Epoch 37/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.3286 - accuracy: 0.4630 - val_loss: 3.2465 - val_accuracy: 0.4902\n",
            "Epoch 38/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.3199 - accuracy: 0.4663 - val_loss: 3.4966 - val_accuracy: 0.4525\n",
            "Epoch 39/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 3.3311 - accuracy: 0.4671 - val_loss: 3.3946 - val_accuracy: 0.4693\n",
            "Epoch 40/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.3348 - accuracy: 0.4654 - val_loss: 3.2992 - val_accuracy: 0.4808\n",
            "Epoch 41/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 3.0525 - accuracy: 0.5271 - val_loss: 3.1249 - val_accuracy: 0.5076\n",
            "Epoch 42/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8925 - accuracy: 0.5405 - val_loss: 2.9661 - val_accuracy: 0.5252\n",
            "Epoch 43/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 2.8237 - accuracy: 0.5446 - val_loss: 2.9176 - val_accuracy: 0.5307\n",
            "Epoch 44/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8101 - accuracy: 0.5446 - val_loss: 3.0149 - val_accuracy: 0.5099\n",
            "Epoch 45/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7817 - accuracy: 0.5468 - val_loss: 3.1597 - val_accuracy: 0.4846\n",
            "Epoch 46/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7628 - accuracy: 0.5513 - val_loss: 2.9066 - val_accuracy: 0.5316\n",
            "Epoch 47/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7700 - accuracy: 0.5463 - val_loss: 3.0456 - val_accuracy: 0.5098\n",
            "Epoch 48/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7769 - accuracy: 0.5492 - val_loss: 2.9192 - val_accuracy: 0.5428\n",
            "Epoch 49/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7814 - accuracy: 0.5520 - val_loss: 3.0077 - val_accuracy: 0.5179\n",
            "Epoch 50/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7688 - accuracy: 0.5559 - val_loss: 2.8942 - val_accuracy: 0.5445\n",
            "Epoch 51/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7814 - accuracy: 0.5544 - val_loss: 2.8643 - val_accuracy: 0.5484\n",
            "Epoch 52/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7854 - accuracy: 0.5571 - val_loss: 2.8434 - val_accuracy: 0.5556\n",
            "Epoch 53/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7789 - accuracy: 0.5597 - val_loss: 2.9653 - val_accuracy: 0.5347\n",
            "Epoch 54/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7776 - accuracy: 0.5630 - val_loss: 2.9116 - val_accuracy: 0.5517\n",
            "Epoch 55/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7998 - accuracy: 0.5636 - val_loss: 3.0075 - val_accuracy: 0.5287\n",
            "Epoch 56/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8127 - accuracy: 0.5618 - val_loss: 3.2204 - val_accuracy: 0.5068\n",
            "Epoch 57/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8152 - accuracy: 0.5652 - val_loss: 2.8434 - val_accuracy: 0.5606\n",
            "Epoch 58/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.7950 - accuracy: 0.5710 - val_loss: 3.0696 - val_accuracy: 0.5277\n",
            "Epoch 59/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8058 - accuracy: 0.5686 - val_loss: 3.0364 - val_accuracy: 0.5372\n",
            "Epoch 60/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.8107 - accuracy: 0.5712 - val_loss: 2.9279 - val_accuracy: 0.5606\n",
            "Epoch 61/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.6029 - accuracy: 0.6143 - val_loss: 2.7158 - val_accuracy: 0.5943\n",
            "Epoch 62/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.4631 - accuracy: 0.6377 - val_loss: 2.7286 - val_accuracy: 0.5867\n",
            "Epoch 63/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.4053 - accuracy: 0.6423 - val_loss: 2.6786 - val_accuracy: 0.5931\n",
            "Epoch 64/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.3690 - accuracy: 0.6448 - val_loss: 2.5604 - val_accuracy: 0.6031\n",
            "Epoch 65/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.3373 - accuracy: 0.6465 - val_loss: 2.8180 - val_accuracy: 0.5607\n",
            "Epoch 66/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.3144 - accuracy: 0.6478 - val_loss: 2.6370 - val_accuracy: 0.5948\n",
            "Epoch 67/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.3071 - accuracy: 0.6483 - val_loss: 2.5519 - val_accuracy: 0.6029\n",
            "Epoch 68/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.3016 - accuracy: 0.6478 - val_loss: 2.5363 - val_accuracy: 0.6083\n",
            "Epoch 69/250\n",
            "390/390 [==============================] - 90s 230ms/step - loss: 2.2958 - accuracy: 0.6482 - val_loss: 2.6841 - val_accuracy: 0.5819\n",
            "Epoch 70/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2778 - accuracy: 0.6547 - val_loss: 2.6509 - val_accuracy: 0.5886\n",
            "Epoch 71/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 2.2866 - accuracy: 0.6493 - val_loss: 2.6326 - val_accuracy: 0.5874\n",
            "Epoch 72/250\n",
            "390/390 [==============================] - 92s 235ms/step - loss: 2.2952 - accuracy: 0.6483 - val_loss: 2.6193 - val_accuracy: 0.5966\n",
            "Epoch 73/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2798 - accuracy: 0.6536 - val_loss: 2.6604 - val_accuracy: 0.5904\n",
            "Epoch 74/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 2.2764 - accuracy: 0.6546 - val_loss: 2.6241 - val_accuracy: 0.5958\n",
            "Epoch 75/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2681 - accuracy: 0.6599 - val_loss: 2.7842 - val_accuracy: 0.5767\n",
            "Epoch 76/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 2.2825 - accuracy: 0.6555 - val_loss: 2.6008 - val_accuracy: 0.6036\n",
            "Epoch 77/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2954 - accuracy: 0.6549 - val_loss: 2.5446 - val_accuracy: 0.6128\n",
            "Epoch 78/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 2.2740 - accuracy: 0.6602 - val_loss: 2.6111 - val_accuracy: 0.6034\n",
            "Epoch 79/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2893 - accuracy: 0.6602 - val_loss: 2.7683 - val_accuracy: 0.5762\n",
            "Epoch 80/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.2820 - accuracy: 0.6621 - val_loss: 2.5805 - val_accuracy: 0.6084\n",
            "Epoch 81/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.1256 - accuracy: 0.6967 - val_loss: 2.5319 - val_accuracy: 0.6237\n",
            "Epoch 82/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 2.0231 - accuracy: 0.7172 - val_loss: 2.4718 - val_accuracy: 0.6320\n",
            "Epoch 83/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.9757 - accuracy: 0.7232 - val_loss: 2.4486 - val_accuracy: 0.6364\n",
            "Epoch 84/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 1.9464 - accuracy: 0.7277 - val_loss: 2.4736 - val_accuracy: 0.6261\n",
            "Epoch 85/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.9320 - accuracy: 0.7278 - val_loss: 2.4282 - val_accuracy: 0.6343\n",
            "Epoch 86/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 1.8928 - accuracy: 0.7335 - val_loss: 2.5160 - val_accuracy: 0.6210\n",
            "Epoch 87/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.8846 - accuracy: 0.7338 - val_loss: 2.3869 - val_accuracy: 0.6411\n",
            "Epoch 88/250\n",
            "390/390 [==============================] - 91s 233ms/step - loss: 1.8711 - accuracy: 0.7355 - val_loss: 2.5284 - val_accuracy: 0.6198\n",
            "Epoch 89/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8652 - accuracy: 0.7346 - val_loss: 2.4352 - val_accuracy: 0.6289\n",
            "Epoch 90/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8486 - accuracy: 0.7386 - val_loss: 2.5295 - val_accuracy: 0.6181\n",
            "Epoch 91/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8394 - accuracy: 0.7382 - val_loss: 2.4543 - val_accuracy: 0.6268\n",
            "Epoch 92/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8319 - accuracy: 0.7421 - val_loss: 2.4207 - val_accuracy: 0.6344\n",
            "Epoch 93/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8430 - accuracy: 0.7350 - val_loss: 2.4940 - val_accuracy: 0.6212\n",
            "Epoch 94/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8297 - accuracy: 0.7397 - val_loss: 2.3560 - val_accuracy: 0.6429\n",
            "Epoch 95/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8258 - accuracy: 0.7404 - val_loss: 2.3763 - val_accuracy: 0.6398\n",
            "Epoch 96/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8303 - accuracy: 0.7363 - val_loss: 2.4651 - val_accuracy: 0.6282\n",
            "Epoch 97/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8290 - accuracy: 0.7402 - val_loss: 2.4225 - val_accuracy: 0.6379\n",
            "Epoch 98/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.8224 - accuracy: 0.7402 - val_loss: 2.4645 - val_accuracy: 0.6292\n",
            "Epoch 99/250\n",
            "390/390 [==============================] - 91s 234ms/step - loss: 1.8150 - accuracy: 0.7415 - val_loss: 2.5500 - val_accuracy: 0.6135\n",
            "Epoch 100/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.8049 - accuracy: 0.7447 - val_loss: 2.4032 - val_accuracy: 0.6409\n",
            "Epoch 101/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.7022 - accuracy: 0.7682 - val_loss: 2.3699 - val_accuracy: 0.6489\n",
            "Epoch 102/250\n",
            "390/390 [==============================] - 92s 236ms/step - loss: 1.6225 - accuracy: 0.7886 - val_loss: 2.4104 - val_accuracy: 0.6419\n",
            "Epoch 103/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.5882 - accuracy: 0.7954 - val_loss: 2.3537 - val_accuracy: 0.6499\n",
            "Epoch 104/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.5587 - accuracy: 0.8004 - val_loss: 2.3030 - val_accuracy: 0.6619\n",
            "Epoch 105/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.5468 - accuracy: 0.8001 - val_loss: 2.3498 - val_accuracy: 0.6541\n",
            "Epoch 106/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.5304 - accuracy: 0.8018 - val_loss: 2.3796 - val_accuracy: 0.6479\n",
            "Epoch 107/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.5150 - accuracy: 0.8038 - val_loss: 2.3238 - val_accuracy: 0.6570\n",
            "Epoch 108/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.4955 - accuracy: 0.8089 - val_loss: 2.3964 - val_accuracy: 0.6453\n",
            "Epoch 109/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.4940 - accuracy: 0.8073 - val_loss: 2.4352 - val_accuracy: 0.6419\n",
            "Epoch 110/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4855 - accuracy: 0.8076 - val_loss: 2.3435 - val_accuracy: 0.6516\n",
            "Epoch 111/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4662 - accuracy: 0.8114 - val_loss: 2.4478 - val_accuracy: 0.6364\n",
            "Epoch 112/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4676 - accuracy: 0.8108 - val_loss: 2.3804 - val_accuracy: 0.6494\n",
            "Epoch 113/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4625 - accuracy: 0.8124 - val_loss: 2.3195 - val_accuracy: 0.6562\n",
            "Epoch 114/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.4475 - accuracy: 0.8129 - val_loss: 2.3379 - val_accuracy: 0.6569\n",
            "Epoch 115/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4491 - accuracy: 0.8123 - val_loss: 2.3560 - val_accuracy: 0.6481\n",
            "Epoch 116/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4354 - accuracy: 0.8159 - val_loss: 2.3180 - val_accuracy: 0.6548\n",
            "Epoch 117/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4305 - accuracy: 0.8153 - val_loss: 2.4113 - val_accuracy: 0.6375\n",
            "Epoch 118/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4269 - accuracy: 0.8174 - val_loss: 2.3889 - val_accuracy: 0.6489\n",
            "Epoch 119/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4072 - accuracy: 0.8207 - val_loss: 2.3669 - val_accuracy: 0.6487\n",
            "Epoch 120/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.4195 - accuracy: 0.8158 - val_loss: 2.3474 - val_accuracy: 0.6525\n",
            "Epoch 121/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 1.3431 - accuracy: 0.8354 - val_loss: 2.2812 - val_accuracy: 0.6624\n",
            "Epoch 122/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.2896 - accuracy: 0.8498 - val_loss: 2.4016 - val_accuracy: 0.6495\n",
            "Epoch 123/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.2668 - accuracy: 0.8556 - val_loss: 2.3292 - val_accuracy: 0.6589\n",
            "Epoch 124/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.2514 - accuracy: 0.8574 - val_loss: 2.3057 - val_accuracy: 0.6636\n",
            "Epoch 125/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.2319 - accuracy: 0.8619 - val_loss: 2.3754 - val_accuracy: 0.6560\n",
            "Epoch 126/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.2353 - accuracy: 0.8586 - val_loss: 2.3180 - val_accuracy: 0.6641\n",
            "Epoch 127/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.2124 - accuracy: 0.8637 - val_loss: 2.2957 - val_accuracy: 0.6623\n",
            "Epoch 128/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.1967 - accuracy: 0.8665 - val_loss: 2.3511 - val_accuracy: 0.6583\n",
            "Epoch 129/250\n",
            "390/390 [==============================] - 90s 231ms/step - loss: 1.1894 - accuracy: 0.8658 - val_loss: 2.3797 - val_accuracy: 0.6573\n",
            "Epoch 130/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.1898 - accuracy: 0.8665 - val_loss: 2.3214 - val_accuracy: 0.6630\n",
            "Epoch 131/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.1815 - accuracy: 0.8673 - val_loss: 2.3560 - val_accuracy: 0.6609\n",
            "Epoch 132/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1643 - accuracy: 0.8709 - val_loss: 2.3733 - val_accuracy: 0.6586\n",
            "Epoch 133/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1622 - accuracy: 0.8721 - val_loss: 2.3664 - val_accuracy: 0.6578\n",
            "Epoch 134/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.1493 - accuracy: 0.8727 - val_loss: 2.3558 - val_accuracy: 0.6608\n",
            "Epoch 135/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1568 - accuracy: 0.8708 - val_loss: 2.3674 - val_accuracy: 0.6587\n",
            "Epoch 136/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1410 - accuracy: 0.8749 - val_loss: 2.3800 - val_accuracy: 0.6586\n",
            "Epoch 137/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.1450 - accuracy: 0.8722 - val_loss: 2.4612 - val_accuracy: 0.6513\n",
            "Epoch 138/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1383 - accuracy: 0.8731 - val_loss: 2.3834 - val_accuracy: 0.6583\n",
            "Epoch 139/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1244 - accuracy: 0.8758 - val_loss: 2.4315 - val_accuracy: 0.6538\n",
            "Epoch 140/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.1228 - accuracy: 0.8755 - val_loss: 2.4112 - val_accuracy: 0.6567\n",
            "Epoch 141/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.0804 - accuracy: 0.8868 - val_loss: 2.3387 - val_accuracy: 0.6706\n",
            "Epoch 142/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.0612 - accuracy: 0.8923 - val_loss: 2.3391 - val_accuracy: 0.6691\n",
            "Epoch 143/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.0372 - accuracy: 0.8963 - val_loss: 2.3548 - val_accuracy: 0.6652\n",
            "Epoch 144/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.0284 - accuracy: 0.8993 - val_loss: 2.3282 - val_accuracy: 0.6721\n",
            "Epoch 145/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.0237 - accuracy: 0.9004 - val_loss: 2.3085 - val_accuracy: 0.6749\n",
            "Epoch 146/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 1.0196 - accuracy: 0.9016 - val_loss: 2.3154 - val_accuracy: 0.6711\n",
            "Epoch 147/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.0077 - accuracy: 0.9035 - val_loss: 2.3445 - val_accuracy: 0.6682\n",
            "Epoch 148/250\n",
            "390/390 [==============================] - 90s 232ms/step - loss: 1.0011 - accuracy: 0.9032 - val_loss: 2.3470 - val_accuracy: 0.6695\n",
            "Epoch 149/250\n",
            "390/390 [==============================] - 91s 232ms/step - loss: 0.9932 - accuracy: 0.9073 - val_loss: 2.3482 - val_accuracy: 0.6701\n",
            "Epoch 150/250\n",
            "309/390 [======================>.......] - ETA: 17s - loss: 0.9821 - accuracy: 0.9087Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fKOUrMJGFs6",
        "colab_type": "text"
      },
      "source": [
        "We observe that our model started overfitting around the epoch 60, but we continued training to achieve the best possible validation loss ~ 68%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ttjgz94Freb",
        "colab_type": "text"
      },
      "source": [
        "Finally evaluating our model on the Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZJpboUzdnZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9f10334f-06d1-45d9-eb78-d33eaaebb463"
      },
      "source": [
        "testing_loss, testing_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('The testing accuracy on CIFAR - 100 Dataset is: ', testing_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 9s 935us/step\n",
            "The testing accuracy on CIFAR - 100 Dataset is:  0.6735000014305115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIuC1mcazRw",
        "colab_type": "text"
      },
      "source": [
        "Finally, we observe that the testing accuracy is 67.35%, which seems pretty decent as the optimum accuracy for VGG 16 is around 71%."
      ]
    }
  ]
}
