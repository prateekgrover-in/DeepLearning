# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dFVQDo2qBZQanAZeSUD53h6rXx5AF-qC
"""

import tensorflow as tf
import numpy as np
import keras

from keras.datasets import cifar100
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()

num_label_classes = 100
input_image_shape = [32,32,3]
batch_size = 32
dim_capsule = 16
num_routings = 3

x = tf.keras.layers.Input(shape=input_image_shape, batch_size=batch_size)

conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)

primary_capsule = tf.keras.layers.Conv2D(filters=256, kernel_size=9, strides=2, padding = 'valid', name='primarycap_conv2d')(conv1)

primary_capsule = tf.keras.layers.Reshape(target_shape=[-1, 8], name='primarycap_reshape')(primary_capsule)

def squash(vectors, axis=-1):
 
    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)
    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + 0.000000000000000000001)
    return scale * vectors

primary_capsule_output = tf.keras.layers.Lambda(squash, name='primarycap_squash')(primary_capsule)

primary_capsule_output.shape[1]

Z = tf.keras.layers.Layer()
W = Z.add_weight(shape=[num_label_classes, primary_capsule_output.shape[1], dim_capsule, primary_capsule_output.shape[2]], initializer=tf.keras.initializers.GlorotUniform)

primary_capsule_output_expanded = tf.expand_dims(tf.expand_dims(primary_capsule_output, 1), -1)

primary_capsule_output_tiled = tf.tile(primary_capsule_output_expanded, [1, num_label_classes, 1, 1, 1])

primary_capsule_output_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(W, x), elems=primary_capsule_output_tiled))

b = tf.zeros(shape=[primary_capsule_output.shape[0], num_label_classes, 1, primary_capsule_output.shape[1]])

for i in range(num_routings):
            c = tf.nn.softmax(b, axis=1)
            outputs = squash(tf.matmul(c, primary_capsule_output_hat))

            if i < num_routings - 1:
                b = b + tf.matmul(outputs, primary_capsule_output_hat, transpose_b=True)

digit_caps = tf.squeeze(outputs, name='digitcaps')

class Length(tf.keras.layers.Layer):

    def call(self, inputs, **kwargs):
        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + 0.0000000000001)

    def compute_output_shape(self, input_shape):
        return input_shape[:-1]

    def get_config(self):
        config = super(Length, self).get_config()
        return config

out_caps = Length(name='capsnet')(digit_caps)

y = tf.keras.layers.Input(shape=(num_label_classes,))

import tensorflow.keras.backend as K

class Mask(tf.keras.layers.Layer):

    def call(self, inputs, **kwargs):
        if type(inputs) is list: 
            assert len(inputs) == 2
            inputs, mask = inputs
        else: 
            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))
            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])

        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))
        return masked

    def compute_output_shape(self, input_shape):
        if type(input_shape[0]) is tuple:  
            return tuple([None, input_shape[0][1] * input_shape[0][2]])
        else: 
            return tuple([None, input_shape[1] * input_shape[2]])

    def get_config(self):
        config = super(Mask, self).get_config()
        return config

masked_by_y = Mask()([digit_caps, y])  
masked = Mask()(digit_caps)

decoder = tf.keras.models.Sequential(name='decoder')
decoder.add(tf.keras.layers.Dense(512, activation='relu', input_dim=16 * num_label_classes))
decoder.add(tf.keras.layers.Dense(1024, activation='relu'))
decoder.add(tf.keras.layers.Dense(np.prod(input_image_shape), activation='sigmoid'))
decoder.add(tf.keras.layers.Reshape(target_shape=input_image_shape, name='out_recon'))

train_model = tf.keras.models.Model([x, y], [out_caps, decoder(masked_by_y)])
eval_model = tf.keras.models.Model(x, [out_caps, decoder(masked)])

noise = tf.keras.layers.Input(shape=(num_label_classes, 16))

noised_digit_caps = tf.keras.layers.Add()([digit_caps, noise])

masked_noised_y = Mask()([noised_digit_caps, y])

manipulate_model = tf.keras.models.Model([x, y, noise], decoder(masked_noised_y))

def margin_loss(y_true, y_pred):
  
    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \
        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))

    return tf.reduce_mean(tf.reduce_sum(L, 1))

log = tf.keras.callbacks.CSVLogger('/log.csv')

checkpoint = tf.keras.callbacks.ModelCheckpoint('/weights-{epoch:02d}.h5', monitor='val_capsnet_acc', save_best_only=True, save_weights_only=True, verbose=1)

lr_decay = tf.keras.callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9 ** epoch))

# compile the model

train_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
                loss=[margin_loss, 'mse'],
                loss_weights=[1., 0.392],
                metrics={'capsnet': 'accuracy'})

train_model.summary()

train_model.fit([x_train, y_train], [y_train, x_train], batch_size=32, epochs=50,
                validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, checkpoint, lr_decay])

model.save_weights('/trained_model.h5')
print('Trained model saved to \'%s/trained_model.h5\'')

from utils import plot_log
plot_log('/log.csv', show=True)